% Question - 1
\begin{Problem}
Give an example each of probability mass functions with finite and infinite ranges. Show that the conditions on PMF are satisfied by your example.
\end{Problem}
\begin{Solution}
Consider the example of the following PMF n: Poisson Random Variable\\
\begin{align*}
    p(n) &= \frac{\lambda^n}{n!}e^{-\lambda} &[\lambda > 0]\\
\end{align*}
This is an example of a probability mass function with an infinite range. As \\ n \rightarrow \infty \\p(n) \rightarrow 0 \\ Hence there are infinitely many values for which p(n) $\neq$ 0 \\
\begin{align*}
    p(n) & \geq 0 &[\since \text{all the terms are positive}]\\
    &{}&{} [\text{condition (1) satisfied}]\\
    \sum_{n=1}^{\infty}p(n) &= \sum_{n=1}^{\infty}\frac{\frac{\lambda^n}{n!}}{e^{\lambda}}\\
    &= \frac{\sum_{n=1}^{\infty}\frac{\lambda^n}{n!}}{e^{\lambda}} &[\since \sum_{n=1}^{\infty}\frac{\lambda^n}{n!} = e^{\lambda}]\\
    &= \frac{e^{\lambda}}{e^{\lambda}} = 1\\
    \therefore \sum_{n=1}^{\infty}p(n) &= 1 &[\text{condition (2) satisfied}]
\end{align*}
Now, consider Another random variable, m, which is the nummber of students in a lab in IIITH. Clearly the range of the random variable is finite since there is a limit to the upper bound $u_b$ = sup(M) and a lower bound $l_b$ = inf(M). The values of the random variable, beyond the region [$l_b$, $u_b$] do not exist. \\
\begin{align*}
    \therefore p(x) &= 0 &\forall x \in [-\infty, l_b)\cup(u_b, \infty]\\
\end{align*}
\begin{align*}
    p(x) &= \frac{\text{Number of Labs with x students}}{\text{Total number of labs}} &[\since \text{all the terms are positive}] \\
    p(x)& \geq 0 &[\text{condition (1) satisfied}]\\
    \sum_{i=l_b}^{u_b} p(i) &= \sum_{i = l_b}^{u_b}\frac{\text{Number of Labs with i students}}{\text{Total Number of Labs}}\\
    &= \frac{\sum_{i = l_b}^{u_b}\text{Number of Labs with i students}}{\text{Total Number of Labs}} \\
    &= \frac{\text{Total Number of Labs}}{\text{Total Number of Labs}}\\
    \sum_{i=l_b}^{u_b} p(i) &= 1 &[\text{condition (2) satisfied}]
\end{align*}
\end{Solution} 


% Question - 2
\begin{Problem}
Show with complete steps that the variance of uniform density is given by equation 10. (Hint: use the expression for variance in equation 5.)
\end{Problem}
\begin{Solution}
We know that the variance of a density is given by $\sigma^2 = E((x-\mu)^2)$ where $\mu$ is the mean of the density.
\begin{align*}   
    \sigma^2 &= \int_{-\infty}^{\infty}((x-\mu^2)).p(x)dx\\
    &= \int_{-\infty}^{\infty}(x^2 - 2\mu x + \mu^2).p(x)dx\\
    &= \int_{-\infty}^{b}(x^2 - 2\mu x + \mu^2).p(x)dx + \int_{b}^{a}(x^2 - 2\mu x + \mu^2).p(x)dx + \int_{a}^{\infty}(x^2 - 2\mu x + \mu^2).p(x)dx\\
    &= 0 + \int_{a}^{b}(x^2 - 2\mu x + \mu^2).p(x)dx + 0 &[\because eq(9)]\\
    &= \int_{a}^{b}(x^2 - 2\mu x + \mu^2).p(x)dx\\
    &= \int_{a}^{b}(x^2).p(x)dx - 2\mu\int_{a}^{b}x.p(x)dx + \mu^2 &[\because eq(3)]\\
    &= \int_{a}^{b}(x^2).p(x)dx - 2\mu*\mu + \mu^2 &[\because eq(9)]\\
    &= \int_{a}^{b} x^2.p(x) - (\frac{a+b}{2})^2\\
    &= \int_{a}^{b} x^2.\frac{1}{b-a}.p(x)dx - \frac{a^2 + 2ab + b^2}{4}\\
    &= \bigg[ \frac{x^3}{3}*\frac{1}{b-a}\bigg]_{b}^{a} - \frac{a^2 + 2ab + b^2}{4}\\
    &= \bigg[ \frac{b^3 - a^3}{3}*\frac{1}{b-a}\bigg] - \frac{a^2 + 2ab + b^2}{4}\\
    &= \bigg[ \frac{(b-a)(b^2 + ab + a^2)}{3}*\frac{1}{b-a}\bigg]- \frac{a^2 + 2ab + b^2}{4}\\
    &= \bigg[ \frac{(b^2 + ab + a^2)}{3}\bigg]- \frac{a^2 + 2ab + b^2}{4}\\
    &= \bigg[ \frac{(4b^2 + 4ab + 4a^2)}{12}\bigg]- \frac{3a^2 + 6ab + 3b^2}{12}\\
    &= \frac{b^2 - 2ab + a^2}{12}\\
    \sigma^2 &= \frac{(b-a)^2}{12}\\
\end{align*}
Hence the variance of uniform density is $\sigma^2 = \frac{(b-a)^2}{12}$
\end{Solution}


% Question - 3
\begin{Problem}
Show examples of two density functions (draw the function plots) that have the same mean and variance, but clearly different distributions. Plot both functions in the same graph with different colours.
\end{Problem}
\begin{Solution}

\end{Solution}







% Question - 4
\begin{Problem}
Show that the alternate expression for variance given in equation 5 holds for discrete random variables as well.
\end{Problem}
\begin{Solution}
The equation for the variance of a discrete random variable is given by eq.4, which is
\begin{align*}
    Var(x) \equiv \sigma^2 &= E[(x-\mu^2)] = \sum_{i=1}^{n}(v_i - \mu)^2p(v_i)\\
    &= \sum_{i=1}^{n} (v_i^2 - 2\mu v_i + \mu^2)\\
    &= \sum_{i=1}^{n} v_i^2 p(v_i) - 2\mu\sum_{i=1}^{n}v_i p(v_i) + \mu^2\sum_{i=1}^{n}p(v_i) \\*&\because \sum_{i=1}^{n}v_i p(v_i) = \mu &\because\sum_{i=1}^{n}p(v_i) = 1\\
    &= \sum_{i=1}^{n} v_i^2 p(v_i) - 2\mu*\mu + \mu^2 &\because \sum_{i=1}^{n}f(v_i)p(v_i) = E(f(x))\\
    &= E[x^2] - \mu^2 &\because E[x] = \mu\\
    \sigma^2&= E[x^2] - (E[x])^2\\
\end{align*}
Hence the expression for variance given in equation 5 holds for discrete random variables as well.
\end{Solution}

%Question - 5
\begin{Problem}
    Prove that the mean and variance of a normal density, N(μ,σ2) are indeed its parameters, $\mu$ and $\sigma^2$.
    \end{Problem}
    \begin{Solution}
    We know that the Normal Function is given by \\
    \begin{align}
        p(x) &= \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{(x-\mu)^2}{2\sigma^2}}
    \end{align}
    Where $\mu$ and $\sigma^2$ are the mean and variance of the distribution respectively.\\\\
    \textbf{Proof of Mean.}
    \begin{align*}
        \mu \equiv E(X) &= \int_{-\infty}^{\infty}x.p(x).dx\\
        &= \int_{-\infty}^{\infty}x.\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{(x-\mu)^2}{2\sigma^2}}.dx & \text{substituting } \frac{x-\mu}{\sigma} = t\\
        &&x = \sigma t + \mu \\
        &&dx = \sigma dt \\ 
        &=\int_{-\infty}^{\infty}\sigma t\frac{1}{\sqrt{2\pi}\cancel{\sigma}}e^{\frac{-t^2}{2}}.\cancel{\sigma}dt  + \int_{-\infty}^{\infty}\mu \frac{1}{\sqrt{2\pi}\cancel{\sigma}}e^{{\frac{-t^2}{2}}}.\cancel{\sigma}dt\\
    \end{align*}
        since the $1^{st}$ term is an odd function with integral limits of -a and it equals to 0. The $2^{nd}$ term is an even function, with similar integral limits, it equals double of the same integral, with limits 0 to a. [source link]
    \begin{align*}
        E(X)&= 0 + 2.\int_{2}^{\infty}\mu \frac{1}{\sqrt{2\pi}\cancel{\sigma}}e^{{\frac{-t^2}{2}}}.\cancel{\sigma}dt\\
        & \text{substituting } \frac{t^2}{2} = y \implies t.dt = dy \implies dt = \frac{dy}{t} = \frac{dy}{\sqrt{2y}} \text{ and } t = \sqrt{2y}\\
        E(X) &=  2.\int_{0}^{\infty}\mu \frac{1}{\sqrt{2\pi}}e^{-y}.\frac{dy}{\sqrt{2y}} \\
        E(X) &= \frac{\cancel{2}\mu}{\cancel{2}.\sqrt{\pi}}\int_{0}^{\infty}y^{-1/2}e^{-y}.dy &[\Gamma(a+1)=\int_0^{\infty}t^ae^{-t}\,dt\,]\\
        &&\text{[where $\Gamma$ is the Gamma function]}\\
        &&\text{[source link]}\\
        E(X)&= \frac{\mu}{\sqrt{\pi}}\Gamma(-1/2 + 1) = \frac{\mu}{\sqrt{\pi}}\Gamma(1/2)&[\because \Gamma(1/2) = \sqrt{\pi}]\\
        E(X) &= \frac{\mu}{\cancel{\sqrt{\pi}}}\cancel{\sqrt{\pi}}\\
    \end{align*}
        \therefore \textit{E(X) = $\mu$}\\
        Hence Proved.\\\\
        
    \textbf{Proof of Variance.}
    We know that the variance of a distribution is given by $\sigma^2 = E[X^2] - E[X]^2$, hence we shall begin by calculating E[X^2].
    
    
    \begin{align*}
        E[X^2] &= \int_{-\infty}^{\infty}x^2.p(x).dx\\
        &= \int_{-\infty}^{\infty}x^2.\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{(x-\mu)^2}{2\sigma^2}}.dx & \text{substituting }\frac{x-\mu}{\sigma} = t\\
        &&x = \sigma t + \mu \\
        &&dx = \sigma dt \\ 
        &= \int_{-\infty}^{\infty} (\sigma t + \mu)^2.\frac{1}{\sqrt{2\pi}\cancel{\sigma}}.e^{-t^2/2}.\cancel{\sigma}.dt\\
        &= \frac{1}{\sqrt{2\pi}}(\int_{-\infty}^{\infty}\mu^2.e^{-t^2/2}.dt + \int_{-\infty}^{\infty}\sigma^2t^2.e^{-t^2/2}.dt - \int_{-\infty}^{\infty}2\sigma\mu t.e^{-t^2/2}.dt)
    \end{align*}
    The $3^{rd}$ term in the equation's RHS, equals to 0, since it is the integral of an odd function, with limits -a to a. The $1^{st}$ and  $2^{nd}$ terms are even functions with limits -a to a, hence are equal to double of the same integral with limits 0 to a. [source link]
    \begin{align*}
        E[X^2] &= \frac{2}{\sqrt{2\pi}}(\int_{0}^{\infty}\mu^2.e^{-t^2/2}.dt + \int_{0}^{\infty}\sigma^2t^2.e^{-t^2/2}.dt)\\
        \text{substituting y = $t^2/2$}&\implies t = \sqrt{2y} \text{ and } dy = t.dt \\
        && dt = \frac{dy}{\sqrt{2y}}\\
        && t = \sqrt{2y}\\
        &= \frac{2}{\sqrt{2\pi}}(\int_{0}^{\infty}\mu^2.e^{-y}.\frac{dy}{\sqrt{2y}} + \int_{0}^{\infty}\sigma^2.2y.e^{-y}.\frac{dy}{\sqrt{2y}})\\
        &= \frac{\cancel{2}}{\cancel{2}\sqrt{\pi}}(\mu^2.\int_{0}^{\infty}e^{-y}.\frac{dy}{\sqrt{y}} + 2\sigma^2.\int_{0}^{\infty}y.e^{-y}.\frac{dy}{\sqrt{y}})\\
        &= \frac{1}{\sqrt{\pi}}(\mu^2.\int_{0}^{\infty}y^{-1/2}.e^{-y}dy + 2\sigma^2.\int_{0}^{\infty}y^{1/2}.e^{-y}.dy) &[\Gamma(a+1)=\int_0^{\infty}t^ae^{-t}\,dt]\\
        &&\text{[where $\Gamma$ is the Gamma function]}\\
        &= \frac{1}{\sqrt{\pi}} (\mu^2.\Gamma(1/2) + 2.\sigma^2.\Gamma(3/2)) &[\because \Gamma(1/2) = \sqrt{\pi}] \\
        &&[\because \Gamma(3/2) = \sqrt{\pi}/2] \\
        &= \frac{1}{\sqrt{\pi}} (\mu^2.\sqrt{\pi} + \cancel{2}.\sigma^2.\frac{\sqrt{\pi}}{\cancel{2}})\\
        &= \frac{\cancel{\sqrt{\pi}}}{\cancel{\sqrt{\pi}}}(\sigma^2 + \mu^2)\\
        E[X^2] &= \sigma^2 + \mu^2\\
        \therefore E[X^2] = \sigma^2 + \mu^2
    \end{align*}
    Substituting in Var(X) = $E[X^2] - E[X]^2 $ \\We know that $E[X] = \mu$
    \begin{align*}
        Var(X) &= E[X^2] - E[X]^2\\
        &= \sigma^2 + \cancel{\mu^2} - \cancel{(\mu^2)}\\
        Var(X) &= \sigma^2
    \end{align*}
    \therefore \textit{Var(X) = $\sigma^2$}\\
        Hence Proved.\\\\
    \end{Solution}
    
    